{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPiojbqPl96HXylGkNscDnY"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8651738,"sourceType":"datasetVersion","datasetId":5182375}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install scikit-optimize","metadata":{"id":"K7aKiorV8d3F","executionInfo":{"status":"ok","timestamp":1718322552899,"user_tz":360,"elapsed":9928,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"b3d65c34-857a-49f7-96d1-d8ba29a4966d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Preview Dataset","metadata":{"id":"GICbhdKA83E_"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom skopt import BayesSearchCV\n","metadata":{"id":"iOsW9sGj8pvq","executionInfo":{"status":"ok","timestamp":1718322552899,"user_tz":360,"elapsed":3,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"execution":{"iopub.status.busy":"2024-06-14T00:18:27.837828Z","iopub.execute_input":"2024-06-14T00:18:27.838254Z","iopub.status.idle":"2024-06-14T00:18:29.543532Z","shell.execute_reply.started":"2024-06-14T00:18:27.838212Z","shell.execute_reply":"2024-06-14T00:18:29.542429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\nfile_path = '/kaggle/input/cancer-prediction-dataset/The_Cancer_data_1500_V2.csv'\ndf = pd.read_csv(file_path)\n\n# Display dataset overview\ndf.describe().T","metadata":{"id":"H_-E2D9U8-sq","executionInfo":{"status":"ok","timestamp":1718322552899,"user_tz":360,"elapsed":3,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"9f29ebe6-d3fd-4aa3-c413-9dd82ccc06ad","execution":{"iopub.status.busy":"2024-06-14T00:20:54.044323Z","iopub.execute_input":"2024-06-14T00:20:54.045269Z","iopub.status.idle":"2024-06-14T00:20:54.091572Z","shell.execute_reply.started":"2024-06-14T00:20:54.045231Z","shell.execute_reply":"2024-06-14T00:20:54.090364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Categorization\n\nWe categorize the features into binary, continuous, and categorical types. This categorization helps in applying appropriate visualization techniques and preprocessing steps specific to each type of feature.\n","metadata":{"id":"WeVVd5J28_jZ"}},{"cell_type":"code","source":"# Feature categorization\nbinary_features = ['Gender', 'Smoking', 'CancerHistory', 'Diagnosis']\ncontinuous_features = ['Age', 'BMI', 'PhysicalActivity', 'AlcoholIntake']\ncategorical_features = ['GeneticRisk']","metadata":{"id":"qtfCr5tQ9B6c","executionInfo":{"status":"ok","timestamp":1718322552899,"user_tz":360,"elapsed":2,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"execution":{"iopub.status.busy":"2024-06-14T00:20:32.905300Z","iopub.execute_input":"2024-06-14T00:20:32.905753Z","iopub.status.idle":"2024-06-14T00:20:32.911507Z","shell.execute_reply.started":"2024-06-14T00:20:32.905720Z","shell.execute_reply":"2024-06-14T00:20:32.910274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizations and EDA\n\nThis section includes various visualizations to understand the distribution of continuous features and the count of binary and categorical features. These visualizations help in identifying patterns, outliers, and potential relationships between features.\n","metadata":{"id":"heunwz9n9Hcy"}},{"cell_type":"markdown","source":"### Histograms and Box Plots for Continuous Features\n\nHistograms provide insights into the distribution of continuous features, while box plots help in identifying outliers and comparing distributions. We use both plots to get a comprehensive understanding of the data.\n","metadata":{"id":"nzAhdyN893ft"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\nfor i, feature in enumerate(continuous_features):\n    # Histogram\n    ax_hist = axes[i // 2, (i % 2) * 2]\n    ax_hist.hist(df[feature].dropna(), bins=30, color='skyblue', edgecolor='black')\n    ax_hist.set_title(f'Histogram of {feature}')\n    \n    # Boxplot\n    ax_box = axes[i // 2, (i % 2) * 2 + 1]\n    ax_box.boxplot(df[feature].dropna(), vert=True, patch_artist=True, boxprops=dict(facecolor='skyblue'))\n    ax_box.set_title(f'Boxplot of {feature}')\n    ax_box.set_xticks([1])\n    ax_box.set_xticklabels([feature])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T00:22:21.626326Z","iopub.execute_input":"2024-06-14T00:22:21.626768Z","iopub.status.idle":"2024-06-14T00:22:23.554878Z","shell.execute_reply.started":"2024-06-14T00:22:21.626737Z","shell.execute_reply":"2024-06-14T00:22:23.553887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bar Charts for Binary Features\n\nBar charts are used to display the count of each category in binary features. This helps in understanding the distribution of these features across the dataset.\n","metadata":{"id":"CenlA4T997SO"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(10, 8))\nfor i, feature in enumerate(binary_features):\n    sns.countplot(data=df, x=feature, ax=axes[i // 2, i % 2], palette='Set2')\n    axes[i // 2, i % 2].set_title(f'Bar Chart of {feature}')\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"IbdF-RKR98q1","executionInfo":{"status":"ok","timestamp":1718322558078,"user_tz":360,"elapsed":1814,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"d1bd3b12-e6cb-4c3c-922f-3f2120c9129d","execution":{"iopub.status.busy":"2024-06-14T00:18:37.795641Z","iopub.execute_input":"2024-06-14T00:18:37.796618Z","iopub.status.idle":"2024-06-14T00:18:38.711859Z","shell.execute_reply.started":"2024-06-14T00:18:37.796565Z","shell.execute_reply":"2024-06-14T00:18:38.710468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bar Chart for Categorical Feature\n\nThis bar chart shows the distribution of the 'GeneticRisk' feature, providing insights into the prevalence of different risk categories in the dataset.\n","metadata":{"id":"0AviMuOZ-AY1"}},{"cell_type":"code","source":"sns.countplot(data=df, x='GeneticRisk', palette='Set2')\nplt.show()\n","metadata":{"id":"20gz0Z0p-CJL","executionInfo":{"status":"ok","timestamp":1718322558517,"user_tz":360,"elapsed":441,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"1f3f2fe2-a168-4646-e9d8-6ce43ebcfeca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Heatmap with Diagnosis\n\nA heatmap is used to visualize the correlation between features and the target variable 'Diagnosis'. This helps in identifying which features have a strong relationship with the target variable.\n","metadata":{"id":"rzNfJfmA-Egg"}},{"cell_type":"code","source":"plt.figure(figsize=(8, 10))\nsns.heatmap(df.corr()[['Diagnosis']].sort_values(by='Diagnosis', ascending=False), annot=True, cmap='Reds', vmin=-1, vmax=1)\nplt.title('Correlation with Diagnosis', color='blue', fontsize=12)\nplt.show()\n","metadata":{"id":"NI-aNmpw-G01","executionInfo":{"status":"ok","timestamp":1718322559220,"user_tz":360,"elapsed":704,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"f8c770e1-36b9-46b9-8a07-6288aa8bb586"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Box Plots for Continuous Features by Diagnosis\n\nBox plots are used to compare the distribution of continuous features between the diagnosed and non-diagnosed groups. This helps in identifying any significant differences in these features between the two groups.\n","metadata":{"id":"ue0IuKFW-JKJ"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfor i, feature in enumerate(continuous_features):\n    sns.boxplot(data=df, x='Diagnosis', y=feature, ax=axes[i // 2, i % 2], palette='Set2')\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"JnIaejHf-Jxa","executionInfo":{"status":"ok","timestamp":1718322562813,"user_tz":360,"elapsed":3594,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"5addda85-9b29-49fc-c8a2-3999ec5c085e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary Statistics:\n\n#### **Age**: Ranges from 20 to 80 years, with a mean of 50.32 years.\n#### **Gender**: Almost equally distributed (0: Male, 1: Female).\n#### **BMI**: Ranges from 15.00 to 39.96, with a mean of 27.51.\n#### **Smoking**: 26.93% are smokers.\n#### **GeneticRisk**: Categorical values ranging from 0 to 2.\n#### **PhysicalActivity**: Ranges from 0.002 to 9.99 hours per week, with a mean of 4.90 hours.\n#### **AlcoholIntake**: Ranges from 0.001 to 4.99 units per week, with a mean of 2.42 units.\n#### **CancerHistory**: 14.4% have a family history of cancer.\n#### **Diagnosis**: 37.13% have been diagnosed with cancer.\n\n### Binary Features (Bar Charts)\n- **Gender**:\n  - The distribution of gender is nearly equal, with slightly more males than females.\n  - Males: 755 (50.33%)\n  - Females: 745 (49.67%)\n  \n- **Smoking**:\n  - A majority of the individuals in the dataset are non-smokers.\n  - Non-Smokers: 1075 (71.67%)\n  - Smokers: 425 (28.33%)\n  \n- **CancerHistory**:\n  - Most individuals do not have a family history of cancer.\n  - No Family History of Cancer: 1275 (85.00%)\n  - Family History of Cancer: 225 (15.00%)\n  \n- **Diagnosis**:\n  - More individuals have not been diagnosed with cancer compared to those who have.\n  - Not Diagnosed with Cancer: 945 (63.00%)\n  - Diagnosed with Cancer: 555 (37.00%)\n\n### Continuous Features (Histograms with KDE)\n- **Age**:\n  - The distribution of age is relatively uniform across different age groups, with a slight increase in frequency around ages 65 and 80. There is a noticeable dip between the ages of 60 and 65.\n  - Mean: ~50 years\n  \n- **BMI**:\n  - The BMI distribution shows a uniform spread with no distinct peaks, suggesting varied body mass indices across the population.\n  - Mean: ~27.5\n  \n- **Physical Activity**:\n  - The distribution of physical activity hours per week is fairly even, indicating a wide range of physical activity levels among individuals.\n  - Mean: ~5 hours per week\n  \n- **Alcohol Intake**:\n  - The alcohol intake distribution is also relatively uniform, with a slight increase in individuals consuming around 2 units per week.\n  - Mean: ~2.5 units per week\n\n### Categorical Feature (Pie Chart)\n- **Genetic Risk**:\n  - The majority of the population falls under 'Low' genetic risk, with 'Medium' and 'High' risk categories being less frequent.\n  - Low Risk: 895 (59.7%)\n  - Medium Risk: 447 (29.8%)\n  - High Risk: 158 (10.5%)\n\n### Conclusions:\n- The dataset seems to have a balanced gender distribution.\n- There is a higher prevalence of non-smokers (71.67%) and individuals without a family history of cancer (85.00%).\n- Cancer diagnoses are less common (37.00%) compared to non-diagnoses (63.00%).\n- The continuous features show a fairly uniform distribution, indicating diverse characteristics within the population.\n- Most individuals have a low genetic risk for cancer (59.7%), with fewer individuals in the medium (29.8%) and high (10.5%) risk categories.\n\nThe data is very evenly balances throughout the set, with exceptions in some of the binary and categorical features. The continous features don't contain any outliers, meaning the data is pretty cleand and doesn't need much preprocessing.\n\nThe model will be built trying to minimize false negatives as much as possible, making recall a priority metric.","metadata":{"id":"jyzcVq1v_SFo"}},{"cell_type":"markdown","source":"# Data Preprocessing\n\nIn this step, we define the target and features, and standardize the continuous features. Standardization is important for models that rely on distance calculations, as it ensures that each feature contributes equally to the distance.\n","metadata":{"id":"Vb8dKDlz-MeX"}},{"cell_type":"code","source":"# Define the target and features\nX = df.drop('Diagnosis', axis=1)\ny = df['Diagnosis']\n\n# Standardize the continuous features\nscaler = StandardScaler()\nX[continuous_features] = scaler.fit_transform(X[continuous_features])\n","metadata":{"id":"7bnne4NP-ep-","executionInfo":{"status":"ok","timestamp":1718322562813,"user_tz":360,"elapsed":2,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Picking Model\n\nWe define and evaluate several models using Stratified k-Fold Cross-Validation. This technique helps in assessing the model's performance more reliably by reducing the impact of data variability.\n","metadata":{"id":"WRDAwW-4-fRm"}},{"cell_type":"code","source":"# Define models for evaluation\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'SVM': SVC(),\n    'k-NN': KNeighborsClassifier(),\n    'Naive Bayes': GaussianNB()\n}\n\n# Evaluate each model using Stratified k-Fold Cross-Validation\nresults = {}\nk = 5\nskf = StratifiedKFold(n_splits=k)\n\nfor model_name, model in models.items():\n    scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n    results[model_name] = {\n        'Accuracy': scores.mean()\n    }\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame(results).T\nprint(results_df)\n","metadata":{"id":"iYVxehJv-hPE","executionInfo":{"status":"ok","timestamp":1718322572947,"user_tz":360,"elapsed":10135,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"c9ebbb8f-0a9e-4997-ea38-5406480ce27a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning for Gradient Boosting Classifier\n\nWe use Bayesian optimization to tune the hyperparameters of the Gradient Boosting Classifier. This method is more efficient than grid search and can lead to better model performance by exploring the parameter space more effectively.\n","metadata":{"id":"Y-SZ98Xb-nr4"}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nparam_space = {\n    'n_estimators': (170, 180),\n    'learning_rate': (0.085, 0.095),\n    'max_depth': [2],\n    'min_samples_split': [5],\n    'min_samples_leaf': [5]\n}\n\nbayes_search = BayesSearchCV(\n    estimator=GradientBoostingClassifier(),\n    search_spaces=param_space,\n    n_iter=20,\n    cv=5,\n    scoring='recall',\n    n_jobs=-1,\n    random_state=42\n)\n\nbayes_search.fit(X_train, y_train)\nbest_params = bayes_search.best_params_\nbest_model = GradientBoostingClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n","metadata":{"id":"cKDu78Ve-o5c","executionInfo":{"status":"ok","timestamp":1718322650540,"user_tz":360,"elapsed":77595,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"1d7275a4-e332-439d-a487-63bc6e778c24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Histogram of False Negatives for Different Thresholds\n\nWe plot a histogram of false negatives for different thresholds to identify the optimal threshold that minimizes false negatives. This is important for our model's evaluation as we aim to reduce the number of missed positive diagnoses.\n","metadata":{"id":"5d9TPkNJ-t_4"}},{"cell_type":"code","source":"# Evaluate on test set with a predefined threshold\ny_prob = best_model.predict_proba(X_test)[:, 1]\nthresholds = np.arange(0.3, 0.61, 0.01)\nmetrics = {'Threshold': [], 'False Negatives': []}\n\nfor threshold in thresholds:\n    y_pred = (y_prob >= threshold).astype(int)\n    fn = confusion_matrix(y_test, y_pred).ravel()[2]  # Get FN count\n    metrics['Threshold'].append(threshold)\n    metrics['False Negatives'].append(fn)\n\nmetrics_df = pd.DataFrame(metrics)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(metrics_df, x='Threshold', weights='False Negatives', bins=len(thresholds), kde=True)\nplt.xlabel('Threshold')\nplt.ylabel('Count of False Negatives')\nplt.title('Histogram of False Negatives for Different Thresholds (0.3 to 0.6)')\nplt.axvline(0.34, color='r', linestyle='--', label='Optimal Threshold: 0.34')\nplt.legend()\nplt.show()\n","metadata":{"id":"Et-wsIYv-vFV","executionInfo":{"status":"ok","timestamp":1718322651118,"user_tz":360,"elapsed":580,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"a94fa8de-93ab-4c51-872d-d252f44c0c48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Model Evaluation with Predefined Threshold\n\nI manually input the threshold that I considered had the best results.","metadata":{"id":"U1WSM3BL-rAj"}},{"cell_type":"code","source":"optimal_threshold = 0.34\ny_pred_final = (y_prob >= optimal_threshold).astype(int)\n\n# Final evaluation\nfinal_test_accuracy = accuracy_score(y_test, y_pred_final)\nfinal_test_precision = precision_score(y_test, y_pred_final)\nfinal_test_recall = recall_score(y_test, y_pred_final)\nfinal_test_f1 = f1_score(y_test, y_pred_final)\nconf_matrix = confusion_matrix(y_test, y_pred_final)\n\nprint(f\"Final Test Accuracy: {final_test_accuracy}\")\nprint(f\"Final Test Precision: {final_test_precision}\")\nprint(f\"Final Test Recall: {final_test_recall}\")\nprint(f\"Final Test F1 Score: {final_test_f1}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\n","metadata":{"id":"iWhnCdxj-shN","executionInfo":{"status":"ok","timestamp":1718322788041,"user_tz":360,"elapsed":187,"user":{"displayName":"Paulo Araya-Santiago","userId":"07633877317905323681"}},"outputId":"06cfd3c3-8c47-4053-defb-dfccedbbc75d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Interpretation of Metrics:\n\n1. **Accuracy**:\n - **0.94** indicates that 94% of the predictions made by the model are correct. This means that out of 100 predictions, 94 are expected to be accurate.\n\n2. **Precision**:\n - **0.897** indicates that when the model predicts a positive diagnosis (cancer diagnosis), it is correct 89.7% of the time. Precision is crucial when the cost of false positives is high.\n\n3. **Recall**:\n - **0.946** indicates that the model correctly identifies 94.6% of actual positive cases (cancer diagnoses). High recall is important in this context because it means fewer false negatives, which is critical in medical diagnoses to ensure that most of the true cases are identified.\n\n4. **F1 Score**:\n - **0.921** is the harmonic mean of precision and recall. It provides a balance between precision and recall and is useful when you need to take both metrics into account.\n\n5. **Confusion Matrix**:\n - **True Positives (TP)**: 105\n   - These are the cases where the model correctly predicted cancer diagnosis (i.e., the model predicted positive, and it was actually positive).\n - **True Negatives (TN)**: 177\n   - These are the cases where the model correctly predicted no cancer diagnosis (i.e., the model predicted negative, and it was actually negative).\n - **False Positives (FP)**: 12\n   - These are the cases where the model incorrectly predicted cancer diagnosis (i.e., the model predicted positive, but it was actually negative). False positives could lead to unnecessary stress and additional tests for patients.\n - **False Negatives (FN)**: 6\n   - These are the cases where the model incorrectly predicted no cancer diagnosis (i.e., the model predicted negative, but it was actually positive). False negatives are critical as they mean missed diagnoses, which could delay treatment for the patient.\n\n### Practical Implications:\n\n- **High Accuracy and Recall**:\n- The modelâ€™s high accuracy and recall suggest it is very effective in identifying individuals with cancer. This means that in a practical scenario, the model will correctly identify most patients who actually have cancer, which is vital for early detection and treatment.\n\n- **Moderate Precision**:\n- While precision is also quite high, there are still some false positives. This indicates that some patients might be incorrectly diagnosed with cancer, leading to unnecessary follow-up tests and anxiety. However, in medical screening, it is often more acceptable to have a higher number of false positives than false negatives.\n\n- **Balanced F1 Score**:\n- The F1 score indicates a good balance between precision and recall. This balance ensures that the model is not only good at identifying true positives but also does not make too many false positive predictions.\n\n### Conclusion:\n\nThe model is highly effective for cancer diagnosis with a high accuracy and recall rate, meaning it successfully identifies the majority of true cancer cases. The slightly lower precision indicates that there will be some false positives, but this is often acceptable in medical contexts where the priority is to minimize missed diagnoses (false negatives). The high F1 score confirms that the model maintains a good balance between precision and recall, making it reliable for practical use in medical diagnostics.\n","metadata":{"id":"SR96oQajCVFL"}},{"cell_type":"code","source":"","metadata":{"id":"LVIZq3mbDshj"},"execution_count":null,"outputs":[]}]}